---
title: "MA678 homework 01"
author: "Diptanshu Singh (dips@bu.edu)"
date: "Septemeber 10, 2018"
output:
  pdf_document: default
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}} 

```{r setup, include=FALSE}
pacman::p_load(ggplot2, knitr, arm, data.table,Cairo, plot3D)
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, tidy=TRUE , global.par = TRUE)
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

# Introduction 

For homework 1 you will fit linear regression models and interpret them. You are welcome to transform the variables as needed.  How to use `lm` should have been covered in your discussion session.  Some of the code are written for you.  Please remove `eval=FALSE` inside the knitr chunk options for the code to run.

This is not intended to be easy so please come see us to get help.

## Data analysis 

### Pyth!

```{r}
gelman_example_dir<-"http://www.stat.columbia.edu/~gelman/arm/examples/"
pyth <- read.table (paste0(gelman_example_dir,"pyth/exercise2.1.dat"),
                    header=T, sep=" ")
```

The folder pyth contains outcome `y` and inputs `x1`, `x2` for 40 data points, with a further 20 points with the inputs but no observed outcome. Save the file to your working directory and read it into R using the `read.table()` function.

1. Use R to fit a linear regression model predicting `y` from `x1`,`x2`, using the first 40 data points in the file. Summarize the inferences and check the fit of your model.
```{r}
#Understanding the data
summary(pyth)
# x1, x2 are continuos variables. Fitting the model without centering or scaling 

#Training Dataset : First 40 data points
y_train <- pyth[c(1:40),c(1)]
x_train <- pyth[c(1:40),c(-1)]
train   <- cbind(x_train, y_train)

#Test Dataste : Last 20 characters 
x_test <- pyth[c(41:60),c(-1)]

#Fitting regression model
lm_1 <- lm(y_train~x_train$x1 + x_train$x2 )
lm_1
```

2. Display the estimated model graphically as in (GH) Figure 3.2.
```{r}
#Drawing a regression plane made by x1 and x2
library(scatterplot3d)
s3dplot<- scatterplot3d(x_train$x1,x_train$x2,y_train)
s3dplot$plane3d(lm_1)
#This model shows the regression plane and the points in 3D space 

#To understand the Variation with respect to 1 variable : 

ggplot(data = train, aes(x1,y_train)) +
  geom_point() +
  geom_abline(intercept = lm_1$coefficients[1] + lm_1$coefficients[3] * mean(train$x2) , slope = lm_1$coefficients[1]  ) + 
  ggtitle("Plot of y with respect to x1 considering average value of x2 ") + ylab("Y")

ggplot(data = train, aes(x2,y_train)) +
  geom_point() +
  geom_abline(intercept = lm_1$coefficients[1] + lm_1$coefficients[2] * mean(train$x1) , slope = lm_1$coefficients[2]  ) + 
  ggtitle("Plot of y with respect to x2 considering average value of x1 ") + ylab("Y")
    
Y <- as.data.frame(cbind(y_train, y_pred = predict(lm_1)))
#Plot of y and y_predicted 
ggplot ( Y , aes (y_pred, y_train)) + 
  geom_point() + 
  geom_quantile()

```

3. Make a residual plot for this model. Do the assumptions appear to be met?
```{r}
#Residual Plot 
res <- resid(lm_1)

ggplot(data=x_train, aes(lm_1$residuals)) + 
  geom_histogram(binwidth = 0.1, color = "black", fill = "cyan4") +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Histogram for Model Residuals") 

#Residual plot is skewed towards the right

```

4. Make predictions for the remaining 20 data points in the file. How confident do you feel about these predictions?
```{r}
#Comparing distribution of x1 and x2 for test and training dataset
x <- rbind(cbind(x_train,c=1),cbind(x_test,c=2))
ggplot(data=x,aes(x=x1,y=x2)) + geom_point(col=x$c) + xlab("Disp") +
  ylab("mpg") + ggtitle("Scatter plot") + theme_bw() + theme(plot.title = element_text(hjust=0.5))

# Since distribution is simillar, we predict using the formula we obtained from training model 
y_test <- lm_1$coefficients[1] + lm_1$coefficients[2]*x_test$x1 + lm_1$coefficients[3]*x_test$x2
y_test
```

After doing this exercise, take a look at Gelman and Nolan (2002, section 9.4) to see where these data came from. (or ask Masanao)

### Earning and height

Suppose that, for a certain population, we can predict log earnings from log height as follows:

- A person who is 66 inches tall is predicted to have earnings of $30,000.
- Every increase of 1% in height corresponds to a predicted increase of 0.8% in earnings.
- The earnings of approximately 95% of people fall within a factor of 1.1 of predicted values.

1. Give the equation the regression line and the residual standard deviation of the regression.  

##### log(earning) = A + B log(height)  
###### B = 0.008/0.01 (For every 1% increase in height, there is 0.8% increase in Y )  
###### A = log(30000) - 0.8 log(66) = 6.957229  
#### Calculating residual standard deviation = Standard deviation in error of Beta 
###### (1.1 - 1) * B = SD_Error * 1.96 
###### 0.1 * 0.8 / 1.96 = SD_Error = 0.0408

2. Suppose the standard deviation of log heights is 5% in this population. What, then, is the $R^2$ of the regression model described here?

###### Var_Error = 0.0408 ^ 2
###### Var_Population =  0.05 ^ 2 
#### $R^2$ = 1 - Var_Error / Var_Population 
###### $R^2$ = 0.3341

### Beauty and student evaluation 

The folder beauty contains data from Hamermesh and Parker (2005) on student evaluations of instructors' beauty and teaching quality for several courses at the University of Texas. The teaching evaluations were conducted at the end of the semester, and the beauty judgments were made later, by six students who had not attended the classes and were not aware of the course evaluations.
```{r}
beauty.data <- read.table (paste0(gelman_example_dir,"beauty/ProfEvaltnsBeautyPublic.csv"), header=T, sep=",")

```

1. Run a regression using beauty (the variable btystdave) to predict course evaluations (courseevaluation), controlling for various other inputs. Display the fitted model graphically, and explaining the meaning of each of the coefficients, along with the residual standard deviation. Plot the residuals versus fitted values.

```{r}
#Both are continuos variables 
x_train <- beauty.data$btystdave
y_train <- beauty.data$courseevaluation
lm_2 <- lm(y_train~x_train)
lm_2
summary(lm_2)
#The R squared value is quite less. It makes sense as the data looks quite spread out
#Graphical representation 
ggplot(beauty.data, aes(x = btystdave, y = courseevaluation)) + 
  geom_point( color = "Dark Green") +
  stat_smooth(method = "lm", col = "blue") +
  xlab("Beauty") + ylab("Course Evaluation") + ggtitle("Linear Regression model")
    
```

2. Fit some other models, including beauty and also other input variables. Consider at least one model with interactions. For each model, state what the predictors are, and what the inputs are, and explain the meaning of each of its coefficients.

```{r}
#In order to compare variables on the same scale: we will center and scale the variables to same value
beauty.data.sc <- as.data.frame(scale(beauty.data))

lm_3 <- lm( courseevaluation ~ . ,beauty.data.sc )
# Stepwise regression model
step.model <- stepAIC(lm_3, direction = "both", 
                      trace = FALSE)
summary(step.model)
#This model has R^2 = 0.89 but there may be overfitting involved. 

#Since class is an important variable as indicated by high value of effect, so we are combining the variables into one
beauty.data$class_sum <- rowSums(beauty.data[,c('class3','class8','class12','class14','class17','class18','class19','class26','class27')])

#Creating a model with variables class_sum, btystdave
lm_4 <- lm(courseevaluation ~ class_sum * btystdave , beauty.data)
summary(lm_4)

```

See also Felton, Mitchell, and Stinson (2003) for more on this topic 
[link](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=426763)

# Conceptula excercises

### On statistical significance.

Note: This is more like a demo to show you that you can get statistically significant result just by random chance. We haven't talked about the significance of the coefficient so we will follow Gelman and use the approximate definition, which is if the estimate is more than 2 sd away from 0 or equivalently, if the z score is bigger than 2 as being "significant".

 ( From Gelman 3.3 ) In this exercise you will simulate two variables that are statistically independent of each other to see what happens when we run a regression of one on the other.  

1. First generate 1000 data points from a normal distribution with mean 0 and standard deviation 1 by typing in R. Generate another variable in the same way (call it var2).

```{r, eval=FALSE}
var1 <- rnorm(1000,0,1)
var2 <- rnorm(1000,0,1)
```

Run a regression of one variable on the other. Is the slope coefficient statistically significant? [absolute value of the z-score(the estimated coefficient of var1 divided by its standard error) exceeds 2]

```{r, eval=FALSE}
fit  <- lm (var2 ~ var1)
z.scores <- coef(fit)[2]/se.coef(fit)[2]
z.scores
```

2. Now run a simulation repeating this process 100 times. This can be done using a loop. From each simulation, save the z-score (the estimated coefficient of var1 divided by its standard error). If the absolute value of the z-score exceeds 2, the estimate is statistically significant. Here is code to perform the simulation:

```{r, eval=FALSE}
z.scores <- rep (NA, 1000) 
for (k in 1:100) {
  var1 <- rnorm (1000,0,1)
  var2 <- rnorm (1000,0,1)
  fit  <- lm (var2 ~ var1)
  z.scores[k] <- coef(fit)[2]/se.coef(fit)[2]
}

sum( abs(z.scores) > 2)
```
How many of these 100 z-scores are statistically significant?  
*6 values are statistically significant*  
What can you say about statistical significance of regression coefficient?  
*Using absolute values of z-score, in 5 times out of 100; the sample selected from the normal distribution was such that some variance in var2 was explained by var1. We can consider that we got 94 sample that actually say that there are not relationship between var1 and var2. The 95% value is also ~ same as the value of probability we get when we use sd = +-2 (95.5%). To test this out, I checked the setup with 1000 runs and z.score > 3. 2 of these were statistically significant, which ~0.2% which was expected.*

### Fit regression removing the effect of other variables

Consider the general multiple-regression equation
$$Y=A+B_1 X_1 + B_2 X_2 +\cdots + B_kX_k+E$$
An alternative procedure for calculating the least-squares coefficient $B_1$ is as follows:

1. Regress $Y$ on $X_2$ through $X_k$, obtaining residuals $E_{Y|2,\dots,k}$.
2. Regress $X_1$ on $X_2$ through $X_k$, obtaining residuals $E_{1|2,\dots,k}$.
3. Regress the residuals $E_{Y|2,\dots,k}$ on the residuals $E_{1|2,\dots,k}$.  The slope for this simple regression is the multiple-regression slope for $X_1$ that is, $B_1$.

(a)  Apply this procedure to the multiple regression of prestige on education, income, and percentage of women in the Canadian occupational prestige data (http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-3E/datasets/Prestige.pdf), confirming that the coefficient for education is properly recovered.

```{r}
fox_data_dir<-"http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-3E/datasets/"
Prestige<-read.table(paste0(fox_data_dir,"Prestige.txt"))
summary(Prestige)
#Prestige is a continuous variable which is equally distributed, hence not using log transformations
#Regression on all variables except education
lm_5 <- lm(prestige ~ income + women + census + type,Prestige)
summary(lm_5)
resi_wo_edu <- Prestige$prestige - predict(lm_5,newdata = Prestige)
# This model explains around 78% of variance in prestige 
# Regression of education on variables other than prestige
lm_6 <- lm(education ~ income + women + census + type,Prestige)
resi_edu <- Prestige$education - predict(lm_6, newdata = Prestige)
#Regressing resi_wo_edu (residual of prestige variable w/o education variable)
#on resi_edu (residual of education variable obtained from model using same predictors) 
lm_7 <- lm(resi_wo_edu ~ resi_edu)
summary(lm_7)
#Slope for this regression is 3.933 

#Regression using all the variables 
lm_8 <- lm(prestige ~ ., Prestige)
summary(lm_8)
#While using regression of prestige on all variables; we get the same coefficient = 3.933

```
(b) The intercept for the simple regression in step 3 is 0.  Why is this the case?  
*The intercept is zero cause the residuals for both resi_wo_edu and resi_education will have average value of zero. And regression line passes thrugh the average of the independent and dependent variables in case of a 1 continuous model*  

(c) In light of this procedure, is it reasonable to describe $B_1$ as the "effect of $X_1$ on $Y$ when the influence of $X_2,\cdots,X_k$ is removed from both $X_1$ and $Y$"?  
*Yes. This factor is helping in understanding the additional variance that only the factor X1 is exlaining*  

(d) The procedure in this problem reduces the multiple regression to a series of simple regressions ( in Step 3). Can you see any practical application for this procedure?  
*We can use it when we want to analyze if the additional of a particular variable is actually helping us explain the data better of not*  

### Partial correlation 

The partial correlation between $X_1$ and $Y$ "controlling for" $X_2,\cdots,X_k$ is defined as the simple correlation between the residuals $E_{Y|2,\dots,k}$ and $E_{1|2,\dots,k}$, given in the previous exercise. The partial correlation is denoted $r_{y1|2,\dots, k}$.

1. Using the Canadian occupational prestige data, calculate the partial correlation between prestige and education, controlling for income and percentage women.

```{r}
lm_9 <- lm(prestige ~ income + women ,Prestige);
resi_wo_edu <- Prestige$prestige - predict(lm_9,newdata = Prestige);
# Regression of education on variables other than prestige
lm_10 <- lm(education ~ income + women ,Prestige);
resi_edu <- Prestige$education - predict(lm_10, newdata = Prestige);

#To find correlation between the residuals
cor(cbind(resi_edu, resi_wo_edu))
#The partial correlation between prestige and education is 0.736
```

2. In light of the interpretation of a partial regression coefficient developed in the previous exercise, why is $r_{y1|2,\dots, k}=0$ if and only if $B_1$ is 0?
*If the residuals are not correlated, it means that residuals of $X_1 will not be able explain the residuals left after Y regressed onto $X_2,\dots,$X_K. *

## Mathematical exercises.

Prove that the least-squares fit in simple-regression analysis has the following properties:

1. $\sum \hat{y}_i\hat{e}_i =0$

2. $\sum (y_i-\hat{y}_i)(\hat{y}_i-\bar{y}) =\sum \hat{e}_i (\hat{y}_i-\bar{y})=0$

Suppose that the means and standard deviations of $\mat{y}$ and  $\mat{x}$ are the same:  $\bar{\mat{y}}=\bar{\mat{x}}$ and $sd(\mat{y})=sd(\mat{x})$.

1. Show that, under these circumstances 
$$\beta_{y|x}=\beta_{x|y}=r_{xy}$$
where $\beta_{y|x}$ is the least-squares slope for the simple regression of $\mat{y}$ on $\mat{x}$, $\beta_{x|y}$ is the least-squares slope for the simple regression of $\mat{x}$ on $\mat{y}$, and $r_{xy}$ is the correlation between the two variables. Show that the intercepts are also the same, $\alpha_{y|x}=\alpha_{x|y}$.

2. Why, if $\alpha_{y|x}=\alpha_{x|y}$ and $\beta_{y|x}=\beta_{x|y}$, is the least squares line for the regression of $\mat{y}$  on $\mat{x}$ different from the line for the regression of $\mat{x}$ on $\mat{y}$ (when $r_{xy}<1$)?

3. Imagine that educational researchers wish to assess the efficacy of a new program to improve the reading performance of children. To test the program, they recruit a group of children who are reading substantially vbelow grade level; after a year in the program, the researchers observe that the children, on average, have imporved their reading performance.  Why is this a weak research design?  How could it be improved?

# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opnions.

